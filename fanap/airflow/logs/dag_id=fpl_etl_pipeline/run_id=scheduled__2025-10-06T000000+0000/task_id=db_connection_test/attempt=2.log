[2025-10-07T00:05:05.085+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: fpl_etl_pipeline.db_connection_test scheduled__2025-10-06T00:00:00+00:00 [queued]>
[2025-10-07T00:05:05.098+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: fpl_etl_pipeline.db_connection_test scheduled__2025-10-06T00:00:00+00:00 [queued]>
[2025-10-07T00:05:05.098+0000] {taskinstance.py:1308} INFO - Starting attempt 2 of 2
[2025-10-07T00:05:05.124+0000] {taskinstance.py:1327} INFO - Executing <Task(PythonOperator): db_connection_test> on 2025-10-06 00:00:00+00:00
[2025-10-07T00:05:05.135+0000] {standard_task_runner.py:57} INFO - Started process 516 to run task
[2025-10-07T00:05:05.140+0000] {standard_task_runner.py:84} INFO - Running: ['airflow', 'tasks', 'run', 'fpl_etl_pipeline', 'db_connection_test', 'scheduled__2025-10-06T00:00:00+00:00', '--job-id', '13', '--raw', '--subdir', 'DAGS_FOLDER/fpl_etl_pipeline.py', '--cfg-path', '/tmp/tmpzeux49ct']
[2025-10-07T00:05:05.143+0000] {standard_task_runner.py:85} INFO - Job 13: Subtask db_connection_test
[2025-10-07T00:05:05.232+0000] {task_command.py:410} INFO - Running <TaskInstance: fpl_etl_pipeline.db_connection_test scheduled__2025-10-06T00:00:00+00:00 [running]> on host 3bca3fdb3451
[2025-10-07T00:05:05.365+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='richardilemon' AIRFLOW_CTX_DAG_ID='fpl_etl_pipeline' AIRFLOW_CTX_TASK_ID='db_connection_test' AIRFLOW_CTX_EXECUTION_DATE='2025-10-06T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-10-06T00:00:00+00:00'
[2025-10-07T00:05:05.387+0000] {base.py:73} INFO - Using connection ID 'fpl_db_conn' for task execution.
[2025-10-07T00:05:05.398+0000] {logging_mixin.py:150} INFO - Connected to database: richardilemon
[2025-10-07T00:05:05.398+0000] {db_config.py:18} INFO - Database Connection Established
[2025-10-07T00:05:05.399+0000] {python.py:183} INFO - Done. Returned value was: <connection object at 0x7f69022df590; dsn: 'user=richardilemon password=xxx dbname='' host=postgres port=5433', closed: 0>
[2025-10-07T00:05:05.412+0000] {xcom.py:640} ERROR - Object of type connection is not JSON serializable. If you are using pickle instead of JSON for XCom, then you need to enable pickle support for XCom in your airflow config or make sure to decorate your object with attr.
[2025-10-07T00:05:05.413+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/json.py", line 91, in default
    return serialize(o)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/serialization/serde.py", line 172, in serialize
    raise TypeError(f"cannot serialize object of type {cls}")
TypeError: cannot serialize object of type <class 'psycopg2.extensions.connection'>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/taskinstance.py", line 2361, in xcom_push
    session=session,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/xcom.py", line 243, in set
    map_index=map_index,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/models/xcom.py", line 632, in serialize_value
    return json.dumps(value, cls=XComEncoder).encode("UTF-8")
  File "/usr/local/lib/python3.7/json/__init__.py", line 238, in dumps
    **kw).encode(obj)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/json.py", line 104, in encode
    return super().encode(o)
  File "/usr/local/lib/python3.7/json/encoder.py", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File "/usr/local/lib/python3.7/json/encoder.py", line 257, in iterencode
    return _iterencode(o, 0)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/json.py", line 93, in default
    return super().default(o)
  File "/usr/local/lib/python3.7/json/encoder.py", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type connection is not JSON serializable
[2025-10-07T00:05:05.429+0000] {taskinstance.py:1350} INFO - Marking task as FAILED. dag_id=fpl_etl_pipeline, task_id=db_connection_test, execution_date=20251006T000000, start_date=20251007T000505, end_date=20251007T000505
[2025-10-07T00:05:05.445+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 13 for task db_connection_test (Object of type connection is not JSON serializable; 516)
[2025-10-07T00:05:05.473+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2025-10-07T00:05:05.507+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
